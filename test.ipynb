{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'File' from 'neptune.types' (c:\\Users\\marie\\anaconda3\\envs\\css\\lib\\site-packages\\neptune\\types\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Accuracy \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfigs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FlairDs\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01measydict\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EasyDict \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marie\\css-peft\\datasets\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflairds\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory_manager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\marie\\css-peft\\datasets\\utils.py:16\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtabulate\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mneptune\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m File \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets, transforms\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'File' from 'neptune.types' (c:\\Users\\marie\\anaconda3\\envs\\css\\lib\\site-packages\\neptune\\types\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import neptune # type: ignore\n",
    "import datetime\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim import SGD, Adam # type: ignore\n",
    "from torch.optim.lr_scheduler import LambdaLR # type: ignore\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "from torchmetrics import Accuracy # type: ignore\n",
    "\n",
    "from configs.utils import *\n",
    "from datasets.utils import *\n",
    "from datasets.utils import FlairDs\n",
    "from easydict import EasyDict # type: ignore\n",
    "from argparse import ArgumentParser, Namespace\n",
    "from model.segmenter_adapt import SegmenterAdapt\n",
    "from dl_toolbox.callbacks import EarlyStopping # type: ignore\n",
    "from model.segmenter_adapt import *\n",
    "from configs.utils import *\n",
    "from datasets.utils import *\n",
    "from easydict import EasyDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    initial_lr=0.01,\n",
    "    final_lr=0.005,\n",
    "    lr_milestones=(20, 80),\n",
    "    epoch_len=100,\n",
    "    sup_batch_size=4,\n",
    "    crop_size=256,\n",
    "    workers=6,\n",
    "    img_aug='d4_rot90_rot270_rot180_d1flip',\n",
    "    max_epochs=200,\n",
    "    sequence_path=\"\",\n",
    "    train_split_coef=0.85,\n",
    "    strategy='continual_{}',\n",
    "    commit=None,\n",
    "    train_type=\"adaptmlp\",\n",
    "    replay=False,  # Par défaut, `store_true` est `False`\n",
    "    config_file=\"/d/maboum/JSTARS/segmentation/configs/config.yml\",\n",
    "    ffn_adapt=True,  # Par défaut, `store_true` est `True`\n",
    "    ffn_num=64,\n",
    "    vpt=False,  # Par défaut, `store_true` est `False`\n",
    "    vpt_num=1,\n",
    "    fulltune=False  # Par défaut, `store_true` est `False`\n",
    ")\n",
    "\n",
    "tuning_config = EasyDict(\n",
    "        # AdaptFormer\n",
    "        ffn_adapt=args.ffn_adapt,\n",
    "        ffn_option=\"parallel\",\n",
    "        ffn_adapter_layernorm_option=\"none\",\n",
    "        ffn_adapter_init_option=\"lora\",\n",
    "        ffn_adapter_scalar=\"0.1\",\n",
    "        ffn_num=args.ffn_num,\n",
    "        d_model=384,\n",
    "        # VPT related\n",
    "        vpt_on=args.vpt,\n",
    "        vpt_num=args.vpt_num,\n",
    "        nb_task = 3\n",
    "    )\n",
    "config_file = \"C:/Users/marie/css-peft/configs/config.yml\"\n",
    "config = load_config_yaml(file_path = config_file)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset = config[\"dataset\"]\n",
    "data_config = dataset[\"flair1\"]\n",
    "seed = config[\"seed\"]\n",
    "directory_path = data_config[\"data_path\"]\n",
    "metadata = data_config[\"metadata\"]\n",
    "data_sequence = data_config[\"task_name\"]\n",
    "epochs = data_config['epochs']\n",
    "eval_freq = data_config['eval_freq']\n",
    "im_size = data_config[\"im_size\"]\n",
    "lr = data_config['learning_rate']\n",
    "win_size = data_config[\"window_size\"]\n",
    "win_stride = data_config[\"window_stride\"]\n",
    "n_channels = data_config['n_channels']\n",
    "n_class = 13\n",
    "class_names = data_config[\"classnames_binary\"]\n",
    "eval_freq = data_config[\"eval_freq\"]\n",
    "\n",
    "selected_model = '_'.join([config[\"model_name\"], \"224\"])\n",
    "model = config[\"model\"]\n",
    "model_config = model[selected_model]\n",
    "im_size = model_config[\"image_size\"]\n",
    "patch_size = model_config[\"patch_size\"]\n",
    "d_model = model_config[\"d_model\"]\n",
    "n_heads = model_config[\"n_heads\"]\n",
    "n_layers = model_config[\"n_layers\"]\n",
    "d_encoder = model_config[\"d_model\"]\n",
    "\n",
    "train_type = args.train_type\n",
    "lora_params = config[\"lora_parameters\"]\n",
    "lora_rank = lora_params[\"rank\"]\n",
    "lora_alpha = lora_params[\"rank\"]\n",
    "\n",
    "binary = data_config[\"binary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = config[\"seed\"]\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.autograd.set_detect_anomaly(True) \n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = neptune.init_run(\n",
    "        project=\"continual-semantic-segmentation/peft-methods\",\n",
    "        api_token = \"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIwN2IzOGYxMC0xYTg5LTQxMGEtYjE3Yy1iNDVkZDM1MmEzYzIifQ==\",\n",
    "        name=\"AdaptFormerSeg\",\n",
    "        description=\"First run for Adapters project\",\n",
    "        tags=[\"adaptmlp\", \"test\", \"segmenter\", \"vit-large\"],\n",
    "        mode= \"debug\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate\n",
    "def lambda_lr(epoch):\n",
    "    m = epoch / args.max_epochs\n",
    "    if m < args.lr_milestones[0]:\n",
    "        return 1\n",
    "    elif m < args.lr_milestones[1]:\n",
    "        return 1 + ((m - args.lr_milestones[0]) / (\n",
    "                    args.lr_milestones[1] - args.lr_milestones[0])) * (\n",
    "                            args.final_lr / args.initial_lr - 1)\n",
    "    else:\n",
    "        return args.final_lr / args.initial_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "segmentation_model = SegmenterAdapt(im_size, n_layers, d_model, d_encoder, 4 * d_model, n_heads, n_class,\n",
    "                                                patch_size, selected_model, tuning_config=tuning_config,\n",
    "                                                model_name=config[\"model_name\"], id = 0).to(device)\n",
    "segmentation_model.load_pretrained_weights()\n",
    "segmentation_model_path = os.path.join(\"C:/Users/marie/css-peft/checkpoints\",args.sequence_path.format(seed),\n",
    "                                 '{}_{}'.format(args.strategy.format(train_type),seed))\n",
    "early_stopping = EarlyStopping(patience=20, verbose=True, \n",
    "                                delta=0.001,path=segmentation_model_path)\n",
    "optimizer = SGD(segmentation_model.parameters(),\n",
    "                lr=args.initial_lr,\n",
    "                momentum=0.9)\n",
    "scheduler = LambdaLR(optimizer,lr_lambda= lambda_lr, verbose = True)\n",
    "loss_fn = torch.nn.CrossEntropyLoss().cuda()\n",
    "accuracy = Accuracy(task='multiclass',num_classes=n_class).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs, test_imgs = [],[]\n",
    "for step,domain in enumerate(data_sequence[:2]):\n",
    "\n",
    "    img = glob.glob(os.path.join(directory_path, '{}/Z*_*/img/IMG_*.tif'.format(domain)))\n",
    "    random.shuffle(img)\n",
    "    train_imgs += img[:int(len(img)*args.train_split_coef)]\n",
    "    test_imgs += img[int(len(img)*args.train_split_coef):]\n",
    "    random.shuffle(train_imgs)\n",
    "    # Train&Validation Data\n",
    "    domain_img_train = train_imgs[:int(len(train_imgs)*args.train_split_coef)]\n",
    "    domain_img_val = train_imgs[int(len(train_imgs)*args.train_split_coef):]\n",
    "    train_loader = create_train_dataloader(domain_img_train, args, data_config, binary= binary)\n",
    "    val_loader = create_val_dataloader(domain_img_val, args, data_config, binary= binary)\n",
    "\n",
    "    for param in segmentation_model.encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "    # Unfreeze the adapt_mlp layers\n",
    "    for name, param in segmentation_model.encoder.named_parameters():\n",
    "        if 'adaptmlp' in name:\n",
    "            param.requires_grad = True\n",
    "\n",
    "    for epoch in range(1,args.max_epochs):\n",
    "        time_ep = time.time()\n",
    "        segmentation_model, train_acc, train_loss = train_function(segmentation_model,train_loader, \n",
    "                                            device,optimizer, loss_fn,\n",
    "                                            accuracy, epoch, data_config, run)\n",
    "        print(train_acc, train_loss)\n",
    "        scheduler.step()\n",
    "        segmentation_model, val_loss, val_acc = validation_function(segmentation_model,val_loader, \n",
    "                                                        device,loss_fn,\n",
    "                                                        accuracy, epoch, data_config, run)\n",
    "        print(val_acc, val_loss)\n",
    "        early_stopping(val_loss,segmentation_model)\n",
    "        if early_stopping.early_stop:\n",
    "            break\n",
    "        time_ep = time.time() - time_ep\n",
    "    segmentation_model.increment()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "css",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
