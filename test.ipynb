{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/maboum/dil_seg/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.15 (you have 1.4.14). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import neptune # type: ignore\n",
    "import datetime\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim import SGD, Adam # type: ignore\n",
    "from torch.optim.lr_scheduler import LambdaLR # type: ignore\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "from torchmetrics import Accuracy # type: ignore\n",
    "\n",
    "from configs.utils import *\n",
    "from datasets.utils import *\n",
    "from datasets.utils import FlairDs\n",
    "from easydict import EasyDict # type: ignore\n",
    "from argparse import ArgumentParser, Namespace\n",
    "from model.segmenter import Segmenter\n",
    "from model.segmenter_adapt import SegmenterAdapt\n",
    "from dl_toolbox.callbacks import EarlyStopping # type: ignore\n",
    "from model.segmenter_adapt import *\n",
    "from configs.utils import *\n",
    "from datasets.utils import *\n",
    "from easydict import EasyDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    initial_lr=0.01,\n",
    "    final_lr=0.005,\n",
    "    lr_milestones=(20, 80),\n",
    "    epoch_len=100,\n",
    "    sup_batch_size=4,\n",
    "    crop_size=256,\n",
    "    workers=6,\n",
    "    img_aug='d4_rot90_rot270_rot180_d1flip',\n",
    "    max_epochs=200,\n",
    "    sequence_path=\"\",\n",
    "    train_split_coef=0.85,\n",
    "    strategy='continual_{}',\n",
    "    commit=None,\n",
    "    train_type=\"adaptmlp\",\n",
    "    replay=False,  # Par défaut, `store_true` est `False`\n",
    "    config_file= \"/d/maboum/css-peft/configs/config.yml\",\n",
    "    ffn_adapt=True,  # Par défaut, `store_true` est `True`\n",
    "    ffn_num=64,\n",
    "    vpt=False,  # Par défaut, `store_true` est `False`\n",
    "    vpt_num=1,\n",
    "    fulltune=False  # Par défaut, `store_true` est `False`\n",
    ")\n",
    "\n",
    "tuning_config = EasyDict(\n",
    "        # AdaptFormer\n",
    "        ffn_adapt=args.ffn_adapt,\n",
    "        ffn_option=\"parallel\",\n",
    "        ffn_adapter_layernorm_option=\"none\",\n",
    "        ffn_adapter_init_option=\"lora\",\n",
    "        ffn_adapter_scalar=\"0.1\",\n",
    "        ffn_num=args.ffn_num,\n",
    "        d_model=768,\n",
    "        # VPT related\n",
    "        vpt_on=args.vpt,\n",
    "        vpt_num=args.vpt_num,\n",
    "        nb_task = 3, \n",
    "        decoder = \"linear\"\n",
    "    )\n",
    "config_file = \"/d/maboum/css-peft/configs/config.yml\"\n",
    "config = load_config_yaml(file_path = config_file)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset = config[\"dataset\"]\n",
    "data_config = dataset[\"flair1\"]\n",
    "seed = config[\"seed\"]\n",
    "directory_path = data_config[\"data_path\"]\n",
    "metadata = data_config[\"metadata\"]\n",
    "data_sequence = data_config[\"task_name\"]\n",
    "epochs = data_config['epochs']\n",
    "eval_freq = data_config['eval_freq']\n",
    "im_size = data_config[\"im_size\"]\n",
    "lr = data_config['learning_rate']\n",
    "win_size = data_config[\"window_size\"]\n",
    "win_stride = data_config[\"window_stride\"]\n",
    "n_channels = data_config['n_channels']\n",
    "n_class = 13\n",
    "class_names = data_config[\"classnames_binary\"]\n",
    "eval_freq = data_config[\"eval_freq\"]\n",
    "\n",
    "selected_model = '_'.join([config[\"model_name\"], \"224\"])\n",
    "model = config[\"model\"]\n",
    "model_config = model[selected_model]\n",
    "im_size = model_config[\"image_size\"]\n",
    "patch_size = model_config[\"patch_size\"]\n",
    "d_model = model_config[\"d_model\"]\n",
    "n_heads = model_config[\"n_heads\"]\n",
    "n_layers = model_config[\"n_layers\"]\n",
    "d_encoder = model_config[\"d_model\"]\n",
    "\n",
    "train_type = args.train_type\n",
    "lora_params = config[\"lora_parameters\"]\n",
    "lora_rank = lora_params[\"rank\"]\n",
    "lora_alpha = lora_params[\"rank\"]\n",
    "\n",
    "binary = data_config[\"binary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = config[\"seed\"]\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.autograd.set_detect_anomaly(True) \n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = neptune.init_run(\n",
    "        project=\"continual-semantic-segmentation/peft-methods\",\n",
    "        api_token = \"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIwN2IzOGYxMC0xYTg5LTQxMGEtYjE3Yy1iNDVkZDM1MmEzYzIifQ==\",\n",
    "        name=\"AdaptFormerSeg\",\n",
    "        description=\"First run for Adapters project\",\n",
    "        tags=[\"adaptmlp\", \"test\", \"segmenter\", \"vit-large\"],\n",
    "        mode= \"debug\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate\n",
    "def lambda_lr(epoch):\n",
    "    m = epoch / args.max_epochs\n",
    "    if m < args.lr_milestones[0]:\n",
    "        return 1\n",
    "    elif m < args.lr_milestones[1]:\n",
    "        return 1 + ((m - args.lr_milestones[0]) / (\n",
    "                    args.lr_milestones[1] - args.lr_milestones[0])) * (\n",
    "                            args.final_lr / args.initial_lr - 1)\n",
    "    else:\n",
    "        return args.final_lr / args.initial_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/maboum/dil_seg/lib/python3.8/site-packages/timm/models/_factory.py:114: UserWarning: Mapping deprecated model name vit_base_patch8_224_dino to current vit_base_patch8_224.dino.\n",
      "  model = create_fn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained model loaded successfully from timm!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/maboum/dil_seg/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "# Callbacks\n",
    "segmentation_model = Segmenter(im_size, n_layers, d_model, d_encoder, 4 * d_model, n_heads, n_class,\n",
    "                                    patch_size, selected_model, model_name=config[\"model_name\"], tuning_config=tuning_config).to(device)\n",
    "\n",
    "segmentation_model.load_pretrained_weights()\n",
    "segmentation_model_path = os.path.join(\"C:/Users/marie/css-peft/checkpoints\",args.sequence_path.format(seed),\n",
    "                                 '{}_{}'.format(args.strategy.format(train_type),seed))\n",
    "\n",
    "optimizer = SGD(segmentation_model.parameters(),\n",
    "                lr=args.initial_lr,\n",
    "                momentum=0.9)\n",
    "scheduler = LambdaLR(optimizer,lr_lambda= lambda_lr, verbose = True)\n",
    "loss_fn = torch.nn.CrossEntropyLoss().cuda()\n",
    "accuracy = Accuracy(task='multiclass',num_classes=n_class).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Segmenter(\n",
       "  (encoder): VisionTransformer(\n",
       "    (patch_embed): PatchEmbedding(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(8, 8), stride=(8, 8))\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): FeedForward(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (head): Linear(in_features=768, out_features=13, bias=True)\n",
       "    (pre_logits): Identity()\n",
       "  )\n",
       "  (decoder): DecoderLinear(\n",
       "    (head): Linear(in_features=768, out_features=13, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segmentation_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('down_proj.weight',\n",
       "              tensor([[ 0.0116, -0.0182,  0.0166,  ..., -0.0267,  0.0017, -0.0172],\n",
       "                      [ 0.0001,  0.0232, -0.0199,  ...,  0.0158,  0.0006,  0.0153],\n",
       "                      [-0.0394,  0.0052, -0.0093,  ..., -0.0301,  0.0264, -0.0039],\n",
       "                      ...,\n",
       "                      [ 0.0187,  0.0441,  0.0388,  ..., -0.0041, -0.0162, -0.0269],\n",
       "                      [ 0.0391,  0.0049, -0.0219,  ..., -0.0109, -0.0306, -0.0196],\n",
       "                      [-0.0197, -0.0351, -0.0327,  ..., -0.0196, -0.0114, -0.0259]],\n",
       "                     device='cuda:0')),\n",
       "             ('down_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('up_proj.weight',\n",
       "              tensor([[-0.0304, -0.0160, -0.0105,  ..., -0.0104, -0.0009,  0.0091],\n",
       "                      [ 0.0121,  0.0417,  0.0052,  ...,  0.0346,  0.0153,  0.0182],\n",
       "                      [-0.0149,  0.0402, -0.0052,  ...,  0.0367,  0.0250,  0.0041],\n",
       "                      ...,\n",
       "                      [ 0.0267,  0.0155,  0.0050,  ...,  0.0152,  0.0044, -0.0411],\n",
       "                      [ 0.0167,  0.0152,  0.0154,  ..., -0.0332, -0.0008,  0.0152],\n",
       "                      [-0.0258,  0.0400, -0.0165,  ..., -0.0006, -0.0114,  0.0017]],\n",
       "                     device='cuda:0')),\n",
       "             ('up_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0'))])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Récupérer le dictionnaire des paramètres\n",
    "params_dict = segmentation_model.encoder.blocks[0].adaptmlp_pool[0].state_dict()\n",
    "\n",
    "params_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupérer les poids et les biais\n",
    "weights = params_dict['up_proj.weight']\n",
    "biases = params_dict['up_proj.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([768, 64]), torch.Size([768]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape, biases.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs, test_imgs = [],[]\n",
    "for step,domain in enumerate(data_sequence[:2]):\n",
    "\n",
    "    img = glob.glob(os.path.join(directory_path, '{}/Z*_*/img/IMG_*.tif'.format(domain)))\n",
    "    random.shuffle(img)\n",
    "    train_imgs += img[:int(len(img)*args.train_split_coef)]\n",
    "    test_imgs += img[int(len(img)*args.train_split_coef):]\n",
    "    random.shuffle(train_imgs)\n",
    "    # Train&Validation Data\n",
    "    domain_img_train = train_imgs[:int(len(train_imgs)*args.train_split_coef)]\n",
    "    domain_img_val = train_imgs[int(len(train_imgs)*args.train_split_coef):]\n",
    "    train_loader = create_train_dataloader(domain_img_train, args, data_config, binary= binary)\n",
    "    val_loader = create_val_dataloader(domain_img_val, args, data_config, binary= binary)\n",
    "\n",
    "    for param in segmentation_model.encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "    # Unfreeze the adapt_mlp layers\n",
    "    for name, param in segmentation_model.encoder.named_parameters():\n",
    "        if 'adaptmlp' in name:\n",
    "            param.requires_grad = True\n",
    "\n",
    "    for epoch in range(1,args.max_epochs):\n",
    "        time_ep = time.time()\n",
    "        segmentation_model, train_acc, train_loss = train_function(segmentation_model,train_loader, \n",
    "                                            device,optimizer, loss_fn,\n",
    "                                            accuracy, epoch, data_config, run)\n",
    "        print(train_acc, train_loss)\n",
    "        scheduler.step()\n",
    "        segmentation_model, val_loss, val_acc = validation_function(segmentation_model,val_loader, \n",
    "                                                        device,loss_fn,\n",
    "                                                        accuracy, epoch, data_config, run)\n",
    "        print(val_acc, val_loss)\n",
    "        early_stopping(val_loss,segmentation_model)\n",
    "        if early_stopping.early_stop:\n",
    "            break\n",
    "        time_ep = time.time() - time_ep\n",
    "    segmentation_model.increment()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "css",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
