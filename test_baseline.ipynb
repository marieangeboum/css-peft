{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91049a35-f142-4989-92cc-1c25a26d4c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft_methods.vit_image import *\n",
    "from easydict import EasyDict\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73de0b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=128, epochs=100, accum_iter=1, model='vit_base_patch16', weight_decay=0.0, lr=None, min_lr=0.0, warmup_epochs=20, finetune='/run/user/108646/gvfs/sftp:host=flexo/d/maboum/AdaptFormer/checkpoints/mae_pretrain_vit_b.pth', global_pool=False, data_path='/datasets01/imagenet_full_size/061417/', nb_classes=1000, output_dir='./output_dir', log_dir=None, device='cuda', seed=0, resume='', strategy='dino', start_epoch=0, eval=False, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, local_rank=-1, dist_on_itp=False, dist_url='env://', dataset='cifar100', drop_path=0.0, inception=False, ffn_adapt=True, ffn_num=64, vpt=False, vpt_num=1, fulltune=False)\n"
     ]
    }
   ],
   "source": [
    "# Manually set the arguments\n",
    "args = argparse.Namespace(\n",
    "    batch_size=128,\n",
    "    epochs=100,\n",
    "    accum_iter=1,\n",
    "    model='vit_base_patch16',\n",
    "    weight_decay=0.0,\n",
    "    lr=None,\n",
    "    min_lr=0.0,\n",
    "    warmup_epochs=20,\n",
    "    finetune='/run/user/108646/gvfs/sftp:host=flexo/d/maboum/AdaptFormer/checkpoints/mae_pretrain_vit_b.pth',\n",
    "    global_pool=False,\n",
    "    data_path='/datasets01/imagenet_full_size/061417/',\n",
    "    nb_classes=1000,\n",
    "    output_dir='./output_dir',\n",
    "    log_dir=None,\n",
    "    device='cuda',\n",
    "    seed=0,\n",
    "    resume='',\n",
    "    strategy = 'dino',\n",
    "    start_epoch=0,\n",
    "    eval=False,\n",
    "    dist_eval=False,\n",
    "    num_workers=10,\n",
    "    pin_mem=True,\n",
    "    world_size=1,\n",
    "    local_rank=-1,\n",
    "    dist_on_itp=False,\n",
    "    dist_url='env://',\n",
    "    dataset='cifar100',\n",
    "    drop_path=0.0,\n",
    "    inception=False,\n",
    "    ffn_adapt=True,\n",
    "    ffn_num=64,\n",
    "    vpt=False,\n",
    "    vpt_num=1,\n",
    "    fulltune=False\n",
    ")\n",
    "\n",
    "print(args)\n",
    "# Add the rest of your script logic here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a8e9934e1298e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune\n",
    "\n",
    "run = neptune.init_run(\n",
    "    project=\"continual-semantic-segmentation/peft-methods\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIwN2IzOGYxMC0xYTg5LTQxMGEtYjE3Yy1iNDVkZDM1MmEzYzIifQ==\",\n",
    ")  # your credentials\n",
    "\n",
    "params = {\"learning_rate\": 0.001, \"optimizer\": \"Adam\"}\n",
    "run[\"parameters\"] = params\n",
    "\n",
    "for epoch in range(10):\n",
    "    run[\"train/loss\"].append(0.9 ** epoch)\n",
    "\n",
    "run[\"eval/f1_score\"] = 0.66\n",
    "\n",
    "run.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebe969cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_config = EasyDict(\n",
    "        # AdaptFormer\n",
    "        ffn_adapt=args.ffn_adapt,\n",
    "        ffn_option=\"parallel\",\n",
    "        ffn_adapter_layernorm_option=\"none\",\n",
    "        ffn_adapter_init_option=\"lora\",\n",
    "        ffn_adapter_scalar=\"0.1\",\n",
    "        ffn_num=args.ffn_num,\n",
    "        d_model=768,\n",
    "        # VPT related\n",
    "        vpt_on=args.vpt,\n",
    "        vpt_num=args.vpt_num,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a18126c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import peft_methods.vit_image as vit_image\n",
    "if args.model.startswith('vit'):\n",
    "        model = vit_image.__dict__[args.model](\n",
    "        num_classes=args.nb_classes,\n",
    "        global_pool=args.global_pool,\n",
    "        drop_path_rate=args.drop_path,\n",
    "        tuning_config=tuning_config,\n",
    "        )\n",
    "else:\n",
    "    raise NotImplementedError(args.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74dfc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa509904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "timm_vision_transformer = timm.create_model('vit_base_patch16_224_dino', pretrained=True)\n",
    "# Obtenir le dictionnaire d'état (state_dict) du modèle\n",
    "state_dict = timm_vision_transformer.state_dict()\n",
    "\n",
    "# Afficher les clés du dictionnaire d'état\n",
    "for key in state_dict.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f851938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupérer les state_dicts\n",
    "state_dict_1 = model.state_dict()\n",
    "state_dict_2 = timm_vision_transformer.state_dict()\n",
    "\n",
    "# Extraire les clés et ignorer celles contenant \"adaptmlp\"\n",
    "keys_model_1 = {key for key in state_dict_1.keys() if \"adaptmlp\" not in key}\n",
    "keys_model_2 = {key for key in state_dict_2.keys() if \"adaptmlp\" not in key}\n",
    "\n",
    "# Clés communes\n",
    "common_keys = keys_model_1 & keys_model_2\n",
    "\n",
    "# Clés différentes\n",
    "diff_keys_1 = keys_model_1 - keys_model_2\n",
    "diff_keys_2 = keys_model_2 - keys_model_1\n",
    "\n",
    "print(\"Clés communes:\")\n",
    "for key in sorted(common_keys):\n",
    "    print(key)\n",
    "\n",
    "print(\"\\nClés présentes uniquement dans le premier modèle:\")\n",
    "for key in sorted(diff_keys_1):\n",
    "    print(key)\n",
    "\n",
    "print(\"\\nClés présentes uniquement dans le modèle timm:\")\n",
    "for key in sorted(diff_keys_2):\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a10174",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = timm_vision_transformer.state_dict()\n",
    "# Spécifiez la clé dont vous voulez connaître la dimension\n",
    "specific_key = 'blocks.0.attn.qkv.bias'\n",
    "\n",
    "# Vérifier si la clé existe dans le state_dict\n",
    "if specific_key in state_dict:\n",
    "    # Récupérer les dimensions du tensor associé à cette clé\n",
    "    weight_tensor = state_dict[specific_key]\n",
    "    dimensions = weight_tensor.size()  # ou weight_tensor.shape\n",
    "    print(f\"Dimensions des poids pour la clé '{specific_key}': {dimensions}\")\n",
    "else:\n",
    "    print(f\"La clé '{specific_key}' n'existe pas dans le state_dict.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459492c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def split_qkv(weights, biases):\n",
    "    \"\"\"\n",
    "    Divise les poids et biais concaténés en poids et biais pour 'query', 'key', et 'value'.\n",
    "    \"\"\"\n",
    "    total_dim = weights.size(0)\n",
    "    assert total_dim % 3 == 0, \"Les dimensions totales des poids ne sont pas divisibles par 3.\"\n",
    "    qkv_dim = total_dim // 3\n",
    "\n",
    "    # Diviser les poids\n",
    "    q_weight = weights[:qkv_dim]\n",
    "    k_weight = weights[qkv_dim:2*qkv_dim]\n",
    "    v_weight = weights[2*qkv_dim:]\n",
    "\n",
    "    # Diviser les biais\n",
    "    q_bias = biases[:qkv_dim]\n",
    "    k_bias = biases[qkv_dim:2*qkv_dim]\n",
    "    v_bias = biases[2*qkv_dim:]\n",
    "    return (q_weight, k_weight, v_weight), (q_bias, k_bias, v_bias)\n",
    "\n",
    "def modify_attn_weights(state_dict):\n",
    "    \"\"\"\n",
    "    Modifie les poids concaténés en poids individuels pour chaque bloc du state_dict.\n",
    "    \"\"\"\n",
    "    new_state_dict = state_dict.copy()\n",
    "\n",
    "    for key in list(state_dict.keys()):\n",
    "        if key.endswith('attn.qkv.weight'):\n",
    "            # Extraire les poids concaténés\n",
    "            qkv_weight = state_dict[key]\n",
    "\n",
    "            # Diviser les poids concaténés\n",
    "            (q_weight, k_weight, v_weight), _ = split_qkv(\n",
    "                qkv_weight,\n",
    "                state_dict[key.replace('.weight', '.bias')]\n",
    "            )\n",
    "\n",
    "            # Créer les nouvelles clés pour les poids\n",
    "            base_key = key.replace('.attn.qkv.weight', '.attn')\n",
    "            new_state_dict[base_key + '.q_proj.weight'] = q_weight\n",
    "            new_state_dict[base_key + '.k_proj.weight'] = k_weight\n",
    "            new_state_dict[base_key + '.v_proj.weight'] = v_weight\n",
    "\n",
    "            # Supprimer la clé originale\n",
    "            del new_state_dict[key]\n",
    "\n",
    "    return new_state_dict\n",
    "\n",
    "def modify_attn_biases(state_dict):\n",
    "    \"\"\"\n",
    "    Modifie les biais concaténés en biais individuels pour chaque bloc du state_dict.\n",
    "    \"\"\"\n",
    "    new_state_dict = state_dict.copy()\n",
    "\n",
    "    for key in list(state_dict.keys()):\n",
    "        if key.endswith('attn.qkv.bias'):\n",
    "            # Extraire les biais concaténés\n",
    "            qkv_bias = state_dict[key]\n",
    "\n",
    "            # Diviser les biais concaténés\n",
    "            _, (q_bias, k_bias, v_bias) = split_qkv(\n",
    "                torch.zeros_like(qkv_bias),  # Les biais sont concaténés, pas de poids associés\n",
    "                qkv_bias\n",
    "            )\n",
    "\n",
    "            # Créer les nouvelles clés pour les biais\n",
    "            base_key = key.replace('.attn.qkv.bias', '.attn')\n",
    "            new_state_dict[base_key + '.q_proj.bias'] = q_bias\n",
    "            new_state_dict[base_key + '.k_proj.bias'] = k_bias\n",
    "            new_state_dict[base_key + '.v_proj.bias'] = v_bias\n",
    "\n",
    "            # Supprimer la clé originale\n",
    "            del new_state_dict[key]\n",
    "\n",
    "    return new_state_dict\n",
    "\n",
    "def remove_proj_keys(state_dict):\n",
    "    \"\"\"\n",
    "    Supprime les clés de poids et biais 'proj' des blocs du state_dict.\n",
    "    \"\"\"\n",
    "    new_state_dict = state_dict.copy()\n",
    "\n",
    "    # Supprimer les clés de poids 'proj'\n",
    "    keys_to_remove = [key for key in new_state_dict.keys() if key.endswith('attn.proj.weight')]\n",
    "    for key in keys_to_remove:\n",
    "        del new_state_dict[key]\n",
    "\n",
    "    # Supprimer les clés de biais 'proj'\n",
    "    keys_to_remove = [key for key in new_state_dict.keys() if key.endswith('attn.proj.bias')]\n",
    "    for key in keys_to_remove:\n",
    "        del new_state_dict[key]\n",
    "\n",
    "    return new_state_dict\n",
    "\n",
    "def modify_state_dict(state_dict):\n",
    "    \"\"\"\n",
    "    Modifie le state_dict pour séparer les poids et biais concaténés en poids et biais individuels\n",
    "    et supprimer les clés 'proj'.\n",
    "    \"\"\"\n",
    "    state_dict = modify_attn_weights(state_dict)\n",
    "    state_dict = modify_attn_biases(state_dict)\n",
    "    state_dict = remove_proj_keys(state_dict)\n",
    "    return state_dict\n",
    "\n",
    "\n",
    "# Obtenir le dictionnaire d'état (state_dict) du modèle\n",
    "state_dict = timm_vision_transformer.state_dict()\n",
    "\n",
    "# Modifier le state_dict\n",
    "modified_state_dict = modify_state_dict(state_dict)\n",
    "for key in modified_state_dict.keys():\n",
    "    print(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddfc6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(modified_state_dict, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752daf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupérer les state_dicts\n",
    "state_dict_1 = model.state_dict()\n",
    "state_dict_2 = modified_state_dict\n",
    "\n",
    "# Extraire les clés et ignorer celles contenant \"adaptmlp\"\n",
    "keys_model_1 = {key for key in state_dict_1.keys() if \"adaptmlp\" not in key}\n",
    "keys_model_2 = {key for key in state_dict_2.keys() if \"adaptmlp\" not in key}\n",
    "\n",
    "# Clés communes\n",
    "common_keys = keys_model_1 & keys_model_2\n",
    "\n",
    "# Clés différentes\n",
    "diff_keys_1 = keys_model_1 - keys_model_2\n",
    "diff_keys_2 = keys_model_2 - keys_model_1\n",
    "\n",
    "print(\"Clés communes:\")\n",
    "for key in sorted(common_keys):\n",
    "    print(key)\n",
    "\n",
    "print(\"\\nClés présentes uniquement dans le premier modèle:\")\n",
    "for key in sorted(diff_keys_1):\n",
    "    print(key)\n",
    "\n",
    "print(\"\\nClés présentes uniquement dans le modèle timm:\")\n",
    "for key in sorted(diff_keys_2):\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dc6967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import timm\n",
    "\n",
    "def split_qkv(weights, biases):\n",
    "    \"\"\"\n",
    "    Divise les poids et biais concaténés en poids et biais pour 'query', 'key', et 'value'.\n",
    "    \"\"\"\n",
    "    total_dim = weights.size(0)\n",
    "    assert total_dim % 3 == 0, \"Les dimensions totales des poids ne sont pas divisibles par 3.\"\n",
    "    qkv_dim = total_dim // 3\n",
    "\n",
    "    # Diviser les poids\n",
    "    q_weight = weights[:qkv_dim]\n",
    "    k_weight = weights[qkv_dim:2*qkv_dim]\n",
    "    v_weight = weights[2*qkv_dim:]\n",
    "\n",
    "    # Diviser les biais\n",
    "    q_bias = biases[:qkv_dim]\n",
    "    k_bias = biases[qkv_dim:2*qkv_dim]\n",
    "    v_bias = biases[2*qkv_dim:]\n",
    "    return (q_weight, k_weight, v_weight), (q_bias, k_bias, v_bias)\n",
    "\n",
    "def modify_attn_weights(state_dict):\n",
    "    \"\"\"\n",
    "    Modifie les poids concaténés en poids individuels pour chaque bloc du state_dict.\n",
    "    \"\"\"\n",
    "    new_state_dict = state_dict.copy()\n",
    "\n",
    "    for key in list(state_dict.keys()):\n",
    "        if key.endswith('attn.qkv.weight'):\n",
    "            # Extraire les poids concaténés\n",
    "            qkv_weight = state_dict[key]\n",
    "\n",
    "            # Diviser les poids concaténés\n",
    "            (q_weight, k_weight, v_weight), _ = split_qkv(\n",
    "                qkv_weight,\n",
    "                state_dict[key.replace('.weight', '.bias')]\n",
    "            )\n",
    "\n",
    "            # Créer les nouvelles clés pour les poids\n",
    "            base_key = key.replace('.attn.qkv.weight', '.attn')\n",
    "            new_state_dict[base_key + '.q_proj.weight'] = q_weight\n",
    "            new_state_dict[base_key + '.k_proj.weight'] = k_weight\n",
    "            new_state_dict[base_key + '.v_proj.weight'] = v_weight\n",
    "\n",
    "            # Supprimer la clé originale\n",
    "            del new_state_dict[key]\n",
    "\n",
    "    return new_state_dict\n",
    "\n",
    "def modify_attn_biases(state_dict):\n",
    "    \"\"\"\n",
    "    Modifie les biais concaténés en biais individuels pour chaque bloc du state_dict.\n",
    "    \"\"\"\n",
    "    new_state_dict = state_dict.copy()\n",
    "\n",
    "    for key in list(state_dict.keys()):\n",
    "        if key.endswith('attn.qkv.bias'):\n",
    "            # Extraire les biais concaténés\n",
    "            qkv_bias = state_dict[key]\n",
    "\n",
    "            # Diviser les biais concaténés\n",
    "            _, (q_bias, k_bias, v_bias) = split_qkv(\n",
    "                torch.zeros_like(qkv_bias),  # Les biais sont concaténés, pas de poids associés\n",
    "                qkv_bias\n",
    "            )\n",
    "\n",
    "            # Créer les nouvelles clés pour les biais\n",
    "            base_key = key.replace('.attn.qkv.bias', '.attn')\n",
    "            new_state_dict[base_key + '.q_proj.bias'] = q_bias\n",
    "            new_state_dict[base_key + '.k_proj.bias'] = k_bias\n",
    "            new_state_dict[base_key + '.v_proj.bias'] = v_bias\n",
    "\n",
    "            # Supprimer la clé originale\n",
    "            del new_state_dict[key]\n",
    "\n",
    "    return new_state_dict\n",
    "\n",
    "\n",
    "def replace_mlp_keys(state_dict):\n",
    "    \"\"\"\n",
    "    Remplace les clés 'mlp.fc1.weight', 'mlp.fc1.bias', 'mlp.fc2.weight', 'mlp.fc2.bias'\n",
    "    par 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias'.\n",
    "    \"\"\"\n",
    "    new_state_dict = state_dict.copy()\n",
    "\n",
    "    for key in list(state_dict.keys()):\n",
    "        if 'mlp.fc' in key:\n",
    "            new_key = key.replace('mlp.fc', 'fc')\n",
    "            new_state_dict[new_key] = new_state_dict.pop(key)\n",
    "\n",
    "    return new_state_dict\n",
    "\n",
    "def modify_state_dict(state_dict):\n",
    "    \"\"\"\n",
    "    Modifie le state_dict pour séparer les poids et biais concaténés en poids et biais individuels,\n",
    "    supprimer les clés 'proj', et remplacer 'mlp.fc*' par 'fc*'.\n",
    "    \"\"\"\n",
    "    state_dict = modify_attn_weights(state_dict)\n",
    "    state_dict = modify_attn_biases(state_dict)\n",
    "    state_dict = replace_mlp_keys(state_dict)\n",
    "    return state_dict\n",
    "\n",
    "\n",
    "# Récupérer le state_dict du modèle\n",
    "state_dict = timm_vision_transformer.state_dict()\n",
    "\n",
    "# Modifier le state_dict\n",
    "modified_state_dict = modify_state_dict(state_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e285f627",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in modified_state_dict.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8140851-5bba-4378-9daa-fd2637e96bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(modified_state_dict, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6c6e00-4923-4f34-b774-470cb429fdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in timm_vision_transformer.state_dict().keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5e0c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "chckpts = torch.load(\"/d/maboum/JSTARS/segmentation/checkpoints/sam_vit_h_4b8939.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2983d218",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in chckpts.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92cf3c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marie\\anaconda3\\envs\\css\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.14 (you have 1.4.13). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'EasyDict' object has no attribute 'nb_task'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 95\u001b[0m\n\u001b[0;32m     91\u001b[0m lora_alpha \u001b[38;5;241m=\u001b[39m lora_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     93\u001b[0m binary \u001b[38;5;241m=\u001b[39m data_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 95\u001b[0m segmentation_model \u001b[38;5;241m=\u001b[39m \u001b[43mSegmenterAdapt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtuning_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtuning_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     98\u001b[0m segmentation_model\u001b[38;5;241m.\u001b[39mload_pretrained_weights()\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m segmentation_model\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mparameters():\n",
      "File \u001b[1;32mc:\\Users\\marie\\css-peft\\model\\segmenter_adapt.py:53\u001b[0m, in \u001b[0;36mSegmenterAdapt.__init__\u001b[1;34m(self, image_size, n_layers, d_model, d_encoder, d_ff, n_heads, n_cls, patch_size, variant, tuning_config, model_name, dropout, drop_path_rate, distilled, channels, id)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mid\u001b[39m\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m \u001b[43mVisionTransformerAdapt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43md_ff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtuning_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdrop_path_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdistilled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchannels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# self.decoder = DecoderLinear(n_cls, patch_size, d_model)\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_pool \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([MaskTransformer(n_cls, patch_size, d_encoder, n_layers,\n\u001b[0;32m     70\u001b[0m                                n_heads, d_model, d_ff, drop_path_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, dropout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m) \n\u001b[0;32m     71\u001b[0m                                \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtuning_config\u001b[38;5;241m.\u001b[39mnb_task)])\n",
      "File \u001b[1;32mc:\\Users\\marie\\css-peft\\model\\vit_image.py:86\u001b[0m, in \u001b[0;36mVisionTransformerAdapt.__init__\u001b[1;34m(self, image_size, patch_size, n_layers, d_model, d_ff, n_heads, n_cls, tuning_config, dropout, drop_path_rate, distilled, channels, task_id)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# transformer blocks\u001b[39;00m\n\u001b[0;32m     84\u001b[0m dpr \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, drop_path_rate, n_layers)]\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[1;32m---> 86\u001b[0m     [Block(dim\u001b[38;5;241m=\u001b[39md_model, num_heads\u001b[38;5;241m=\u001b[39mn_heads, mlp_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4.\u001b[39m, qkv_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     87\u001b[0m            drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.\u001b[39m, attn_drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.\u001b[39m, drop_path\u001b[38;5;241m=\u001b[39mdpr[i], config\u001b[38;5;241m=\u001b[39mtuning_config, layer_id\u001b[38;5;241m=\u001b[39mi, \n\u001b[0;32m     88\u001b[0m            nb_task\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtuning_config\u001b[38;5;241m.\u001b[39mnb_task, task_id\u001b[38;5;241m=\u001b[39mtask_id) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_layers)]\n\u001b[0;32m     89\u001b[0m )\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tuning_config \u001b[38;5;129;01mand\u001b[39;00m tuning_config\u001b[38;5;241m.\u001b[39mvpt_on:\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m tuning_config\u001b[38;5;241m.\u001b[39mvpt_num \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, tuning_config\u001b[38;5;241m.\u001b[39mvpt_num\n",
      "File \u001b[1;32mc:\\Users\\marie\\css-peft\\model\\vit_image.py:88\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# transformer blocks\u001b[39;00m\n\u001b[0;32m     84\u001b[0m dpr \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, drop_path_rate, n_layers)]\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[0;32m     86\u001b[0m     [Block(dim\u001b[38;5;241m=\u001b[39md_model, num_heads\u001b[38;5;241m=\u001b[39mn_heads, mlp_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4.\u001b[39m, qkv_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     87\u001b[0m            drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.\u001b[39m, attn_drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.\u001b[39m, drop_path\u001b[38;5;241m=\u001b[39mdpr[i], config\u001b[38;5;241m=\u001b[39mtuning_config, layer_id\u001b[38;5;241m=\u001b[39mi, \n\u001b[1;32m---> 88\u001b[0m            nb_task\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtuning_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnb_task\u001b[49m, task_id\u001b[38;5;241m=\u001b[39mtask_id) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_layers)]\n\u001b[0;32m     89\u001b[0m )\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tuning_config \u001b[38;5;129;01mand\u001b[39;00m tuning_config\u001b[38;5;241m.\u001b[39mvpt_on:\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m tuning_config\u001b[38;5;241m.\u001b[39mvpt_num \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, tuning_config\u001b[38;5;241m.\u001b[39mvpt_num\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'EasyDict' object has no attribute 'nb_task'"
     ]
    }
   ],
   "source": [
    "from model.segmenter_adapt import *\n",
    "from configs.utils import *\n",
    "from datasets.utils import *\n",
    "import argparse\n",
    "from easydict import EasyDict\n",
    "args_ = argparse.Namespace(\n",
    "    batch_size=128,\n",
    "    epochs=100,\n",
    "    accum_iter=1,\n",
    "    model='vit_base_patch16',\n",
    "    weight_decay=0.0,\n",
    "    lr=None,\n",
    "    min_lr=0.0,\n",
    "    warmup_epochs=20,\n",
    "    finetune='/run/user/108646/gvfs/sftp:host=flexo/d/maboum/AdaptFormer/checkpoints/mae_pretrain_vit_b.pth',\n",
    "    global_pool=False,\n",
    "    data_path='/datasets01/imagenet_full_size/061417/',\n",
    "    nb_classes=1000,\n",
    "    output_dir='./output_dir',\n",
    "    log_dir=None,\n",
    "    device='cuda',\n",
    "    seed=0,\n",
    "    resume='',\n",
    "    strategy = 'dino',\n",
    "    start_epoch=0,\n",
    "    eval=False,\n",
    "    dist_eval=False,\n",
    "    num_workers=10,\n",
    "    pin_mem=True,\n",
    "    world_size=1,\n",
    "    local_rank=-1,\n",
    "    dist_on_itp=False,\n",
    "    dist_url='env://',\n",
    "    dataset='cifar100',\n",
    "    drop_path=0.0,\n",
    "    inception=False,\n",
    "    ffn_adapt=True,\n",
    "    ffn_num=64,\n",
    "    vpt=False,\n",
    "    vpt_num=1,\n",
    "    fulltune=False, \n",
    "    train_type = \"finetuning\",\n",
    ")\n",
    "\n",
    "tuning_config = EasyDict(\n",
    "        # AdaptFormer\n",
    "        ffn_adapt=args_.ffn_adapt,\n",
    "        ffn_option=\"parallel\",\n",
    "        ffn_adapter_layernorm_option=\"none\",\n",
    "        ffn_adapter_init_option=\"lora\",\n",
    "        ffn_adapter_scalar=\"0.1\",\n",
    "        ffn_num=args_.ffn_num,\n",
    "        d_model=768,\n",
    "        # VPT related\n",
    "        vpt_on=args_.vpt,\n",
    "        vpt_num=args_.vpt_num,\n",
    "    )\n",
    "config_file = \"C:/Users/marie/css-peft/configs/config.yml\"\n",
    "config = load_config_yaml(file_path = config_file)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset = config[\"dataset\"]\n",
    "data_config = dataset[\"flair1\"]\n",
    "seed = config[\"seed\"]\n",
    "directory_path = data_config[\"data_path\"]\n",
    "metadata = data_config[\"metadata\"]\n",
    "data_sequence = data_config[\"task_name\"]\n",
    "epochs = data_config['epochs']\n",
    "eval_freq = data_config['eval_freq']\n",
    "im_size = data_config[\"im_size\"]\n",
    "lr = data_config['learning_rate']\n",
    "win_size = data_config[\"window_size\"]\n",
    "win_stride = data_config[\"window_stride\"]\n",
    "n_channels = data_config['n_channels']\n",
    "n_class = data_config[\"n_cls\"]\n",
    "class_names = data_config[\"classnames_binary\"]\n",
    "eval_freq = data_config[\"eval_freq\"]\n",
    "\n",
    "selected_model = '_'.join([config[\"model_name\"], \"224\"])\n",
    "model = config[\"model\"]\n",
    "model_config = model[selected_model]\n",
    "im_size = model_config[\"image_size\"]\n",
    "patch_size = model_config[\"patch_size\"]\n",
    "d_model = model_config[\"d_model\"]\n",
    "n_heads = model_config[\"n_heads\"]\n",
    "n_layers = model_config[\"n_layers\"]\n",
    "d_encoder = model_config[\"d_model\"]\n",
    "\n",
    "train_type = args_.train_type\n",
    "lora_params = config[\"lora_parameters\"]\n",
    "lora_rank = lora_params[\"rank\"]\n",
    "lora_alpha = lora_params[\"rank\"]\n",
    "\n",
    "binary = data_config[\"binary\"]\n",
    "\n",
    "segmentation_model = SegmenterAdapt(im_size, n_layers, d_model, d_encoder, 4 * d_model, n_heads, n_class,\n",
    "                                            patch_size, selected_model, tuning_config=tuning_config,\n",
    "                                            model_name=config[\"model_name\"]).to(device)\n",
    "segmentation_model.load_pretrained_weights()\n",
    "for param in segmentation_model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "# Unfreeze the adapt_mlp layers\n",
    "for name, param in segmentation_model.encoder.named_parameters():\n",
    "    if 'adaptmlp' in name:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0281ef59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SegmenterAdapt(\n",
       "  (encoder): VisionTransformerAdapt(\n",
       "    (patch_embed): PatchEmbedding(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (mlp_drop): Dropout(p=0.0, inplace=False)\n",
       "        (adaptmlp): Adapter(\n",
       "          (down_proj): Linear(in_features=768, out_features=64, bias=True)\n",
       "          (non_linear_func): ReLU()\n",
       "          (up_proj): Linear(in_features=64, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (head): Linear(in_features=768, out_features=13, bias=True)\n",
       "    (pre_logits): Identity()\n",
       "  )\n",
       "  (decoder): MaskTransformer(\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): FeedForward(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (proj_dec): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (decoder_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mask_norm): LayerNorm((13,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segmentation_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
